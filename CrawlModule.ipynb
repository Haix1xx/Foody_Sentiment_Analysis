{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrowserOption(Enum):\n",
    "    \"\"\"Option for webbrowser\n",
    "    \"\"\"\n",
    "    EDGE = 1\n",
    "    CHROME = 2\n",
    "    FIREFOX = 3\n",
    "    SAFARI = 4\n",
    "\n",
    "\n",
    "\n",
    "class FileHandler():\n",
    "    @staticmethod\n",
    "    def is_file_empty(file_name: str) -> bool:\n",
    "        \"\"\"Return True if file is empty\n",
    "\n",
    "        Args: \n",
    "            - file_name: file's name that is needed to be check\n",
    "        \"\"\"\n",
    "        return os.stat(file_name).st_size == 0\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_csv(reviews: list[WebElement], points: list[WebElement], file_name: str) -> None:\n",
    "        header = ['Review', 'Point']\n",
    "        file = open(file_name, 'a', encoding='UTF8', newline='')\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        if FileHandler.is_file_empty(file_name):\n",
    "            writer.writerow(header)\n",
    "        for review, point in zip(reviews, points):\n",
    "            row = [review.text, point.text]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "class FoodyCrawler():\n",
    "    @staticmethod\n",
    "    def get_driver(browser_option: BrowserOption = BrowserOption.CHROME):\n",
    "        \"\"\"Return driver depended on BrowserOption Enum\n",
    "        \n",
    "        Args:\n",
    "            - browser_option: the option of browser's driver\n",
    "        \"\"\"\n",
    "        if browser_option == BrowserOption.EDGE:\n",
    "            options = webdriver.EdgeOptions()\n",
    "            options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "            options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "            return webdriver.ChromiumEdge(options=options)\n",
    "        elif browser_option == BrowserOption.FIREFOX:\n",
    "            return webdriver.Firefox()\n",
    "        elif browser_option == BrowserOption.SAFARI:\n",
    "            return webdriver.Safari()       \n",
    "        else:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "            options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "            return webdriver.Chrome(options=options)  \n",
    "\n",
    "\n",
    "    def __init__(self, browser_option: BrowserOption, url_file: str, file_name_to_save: str, sample_size: int=None) -> None:\n",
    "        \"\"\"Create a new instance of FoodyCrawler\n",
    "        \n",
    "        Args:\n",
    "            - browser_option: the option of browser's driver\n",
    "            - url_file: a file that stores a list of url linked to restaurants\n",
    "            - file_name_to_save: file's name to save data crawled from links in url_file\n",
    "            - n_sample: maximum data can be crawled. If n_sample=None, no limited data\n",
    "        \"\"\"\n",
    "        self.driver = FoodyCrawler.get_driver(browser_option)\n",
    "        self.url_file = url_file\n",
    "        self.file_name_to_save = file_name_to_save\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "\n",
    "    def crawl(self, url: str) -> tuple[list[WebElement], list[WebElement]]:\n",
    "        \"\"\"Crawl data from single url\n",
    "        \n",
    "        Args:\n",
    "            - url: a url linked to a restaurant\n",
    "        \"\"\"\n",
    "        url = url+'/binh-luan'\n",
    "        self.driver.get(url)\n",
    "        self.driver.maximize_window()\n",
    "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                view_review_button = self.driver.find_element(By.PARTIAL_LINK_TEXT, \"Xem thêm bình luận\")\n",
    "                self.driver.execute_script(\"arguments[0].click()\", view_review_button)\n",
    "                time.sleep(1)\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView();\", view_review_button)\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "        review_selector = \"div.review-des > div.rd-des > span\"\n",
    "        point_selector = \"div.review-user > div > div.review-points > span\"\n",
    "\n",
    "        reviews = self.driver.find_elements(By.CSS_SELECTOR, review_selector)\n",
    "        points = self.driver.find_elements(By.CSS_SELECTOR, point_selector)\n",
    "        return reviews, points, len(reviews)\n",
    "    \n",
    "\n",
    "    def start_crawling(self) -> None:\n",
    "        \"\"\"Start crawling data from url_link\"\"\"\n",
    "        with open(self.url_file, 'r') as file:\n",
    "            url_list = file.readlines()\n",
    "            for url in url_list:\n",
    "                print('Crawling data from link: ' + url)\n",
    "                try:\n",
    "                    reviews, points, sample_count = self.crawl(url)\n",
    "                except:\n",
    "                    print('Link not found')\n",
    "                    continue\n",
    "\n",
    "                # check if enough data has been crawled\n",
    "                if self.sample_size is not None:\n",
    "                    if self.sample_size <= sample_count:\n",
    "                        reviews = reviews[:self.sample_size]\n",
    "                        points = points[:self.sample_size]\n",
    "\n",
    "                FileHandler.write_to_csv(reviews, points, self.file_name_to_save)\n",
    "\n",
    "                \n",
    "                if self.sample_size is not None:\n",
    "                    # break if enough\n",
    "                    self.sample_size -= sample_count\n",
    "                    if self.sample_size <= 0:\n",
    "                        break\n",
    "                \n",
    "                self.driver.execute_script(\"window.open('');\")\n",
    "                self.driver.close()\n",
    "                self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foody = FoodyCrawler(browser_option=BrowserOption.EDGE, url_file='restaurants.txt', file_name_to_save='data_1.csv', n_sample=10)\n",
    "foody.start_crawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling data from link: https://www.foody.vn/da-nang/shinshu-chao-ech-singapore-com-ngon-nguyen-hoang\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foody = FoodyCrawler(browser_option=BrowserOption.EDGE, url_file='restaurants.txt', file_name_to_save='test.csv', sample_size=10)\n",
    "foody.start_crawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cháo ếch đậm đà, cơm thì không đặc sắc mấy. \\n...</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quán này mình cũng hay order lắm. Menu quán cũ...</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cháo thịt bò 30k\\n------\\nĐang mệt người vì th...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shinshu đã chuyển qua nguỹen hoàng, vị trí cũn...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chất lượng : Menu đa dạng , nhiều lựa chọn , t...</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Đây là cơ sở cháo Ếch địa chỉ***chuyển về nè. ...</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tăng giá mà chất lượng không tăng. Cơm chừng đ...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mình ăn quán này từ hồi bên***nay quán đã dời ...</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Trời mưa này đặt món cơm ruốc này về ăn ngon k...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cơm thố hải sản, cháo ếch singapore quá ngon, ...</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Point\n",
       "0  Cháo ếch đậm đà, cơm thì không đặc sắc mấy. \\n...    7.4\n",
       "1  Quán này mình cũng hay order lắm. Menu quán cũ...    7.8\n",
       "2  Cháo thịt bò 30k\\n------\\nĐang mệt người vì th...    8.0\n",
       "3  Shinshu đã chuyển qua nguỹen hoàng, vị trí cũn...    8.0\n",
       "4  Chất lượng : Menu đa dạng , nhiều lựa chọn , t...    8.2\n",
       "5  Đây là cơ sở cháo Ếch địa chỉ***chuyển về nè. ...    9.4\n",
       "6  Tăng giá mà chất lượng không tăng. Cơm chừng đ...    1.8\n",
       "7  Mình ăn quán này từ hồi bên***nay quán đã dời ...    8.2\n",
       "8  Trời mưa này đặt món cơm ruốc này về ăn ngon k...   10.0\n",
       "9  Cơm thố hải sản, cháo ếch singapore quá ngon, ...    9.6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv('test.csv')\n",
    "review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ffd7eb2cebf9ac436b5021ba01877e9cee6b03524e01bf8c8637d3e64111215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
