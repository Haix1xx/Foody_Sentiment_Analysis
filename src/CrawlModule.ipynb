{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (4.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from selenium) (2.0.2)\n",
      "Requirement already satisfied: trio~=0.17 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\documents\\semester 6\\data science\\final\\.venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrowserOption(Enum):\n",
    "    \"\"\"Option for webbrowser\n",
    "    \"\"\"\n",
    "    EDGE = 1\n",
    "    CHROME = 2\n",
    "    FIREFOX = 3\n",
    "    SAFARI = 4\n",
    "\n",
    "\n",
    "\n",
    "class FileHandler():\n",
    "    @staticmethod\n",
    "    def is_file_empty(file_name: str) -> bool:\n",
    "        \"\"\"Return True if file is empty\n",
    "\n",
    "        Args: \n",
    "            - file_name: file's name that is needed to be check\n",
    "        \"\"\"\n",
    "        return os.stat(file_name).st_size == 0\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_csv(reviews: list[WebElement], points: list[WebElement], file_name: str) -> None:\n",
    "        header = ['Review', 'Point']\n",
    "        file = open(file_name, 'a', encoding='UTF8', newline='')\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        if FileHandler.is_file_empty(file_name):\n",
    "            writer.writerow(header)\n",
    "        for review, point in zip(reviews, points):\n",
    "            row = [review.text, point.text]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "class FoodyCrawler():\n",
    "    @staticmethod\n",
    "    def get_driver(browser_option: BrowserOption = BrowserOption.CHROME):\n",
    "        \"\"\"Return driver depended on BrowserOption Enum\n",
    "        \n",
    "        Args:\n",
    "            - browser_option: the option of browser's driver\n",
    "        \"\"\"\n",
    "        if browser_option == BrowserOption.EDGE:\n",
    "            options = webdriver.EdgeOptions()\n",
    "            options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "            options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "            return webdriver.ChromiumEdge(options=options)\n",
    "        elif browser_option == BrowserOption.FIREFOX:\n",
    "            return webdriver.Firefox()\n",
    "        elif browser_option == BrowserOption.SAFARI:\n",
    "            return webdriver.Safari()       \n",
    "        else:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "            options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "            return webdriver.Chrome(options=options)  \n",
    "\n",
    "\n",
    "    def __init__(self, browser_option: BrowserOption, url_file: str, file_name_to_save: str, sample_size: int=None) -> None:\n",
    "        \"\"\"Create a new instance of FoodyCrawler\n",
    "        \n",
    "        Args:\n",
    "            - browser_option: the option of browser's driver\n",
    "            - url_file: a file that stores a list of url linked to restaurants\n",
    "            - file_name_to_save: file's name to save data crawled from links in url_file\n",
    "            - n_sample: maximum data can be crawled. If n_sample=None, no limited data\n",
    "        \"\"\"\n",
    "        self.driver = FoodyCrawler.get_driver(browser_option)\n",
    "        self.url_file = url_file\n",
    "        self.file_name_to_save = file_name_to_save\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "\n",
    "    def crawl(self, url: str) -> tuple[list[WebElement], list[WebElement]]:\n",
    "        \"\"\"Crawl data from single url\n",
    "        \n",
    "        Args:\n",
    "            - url: a url linked to a restaurant\n",
    "        \"\"\"\n",
    "        url = url+'/binh-luan'\n",
    "        self.driver.get(url)\n",
    "        self.driver.maximize_window()\n",
    "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                view_review_button = self.driver.find_element(By.PARTIAL_LINK_TEXT, \"Xem thêm bình luận\")\n",
    "                self.driver.execute_script(\"arguments[0].click()\", view_review_button)\n",
    "                time.sleep(1)\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView();\", view_review_button)\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "        review_selector = \"div.review-des > div.rd-des > span\"\n",
    "        point_selector = \"div.review-user > div > div.review-points > span\"\n",
    "\n",
    "        reviews = self.driver.find_elements(By.CSS_SELECTOR, review_selector)\n",
    "        points = self.driver.find_elements(By.CSS_SELECTOR, point_selector)\n",
    "        return reviews, points, len(reviews)\n",
    "    \n",
    "\n",
    "    def start_crawling(self) -> None:\n",
    "        \"\"\"Start crawling data from url_link\"\"\"\n",
    "        with open(self.url_file, 'r') as file:\n",
    "            url_list = file.readlines()\n",
    "            for url in url_list:\n",
    "                print('Crawling data from link: ' + url)\n",
    "                try:\n",
    "                    reviews, points, sample_count = self.crawl(url)\n",
    "                except:\n",
    "                    print('Link not found')\n",
    "                    continue\n",
    "\n",
    "                # check if enough data has been crawled\n",
    "                if self.sample_size is not None:\n",
    "                    if self.sample_size <= sample_count:\n",
    "                        reviews = reviews[:self.sample_size]\n",
    "                        points = points[:self.sample_size]\n",
    "\n",
    "                FileHandler.write_to_csv(reviews, points, self.file_name_to_save)\n",
    "\n",
    "                \n",
    "                if self.sample_size is not None:\n",
    "                    # break if enough\n",
    "                    self.sample_size -= sample_count\n",
    "                    if self.sample_size <= 0:\n",
    "                        break\n",
    "                \n",
    "                self.driver.execute_script(\"window.open('');\")\n",
    "                self.driver.close()\n",
    "                self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foody = FoodyCrawler(browser_option=BrowserOption.EDGE, url_file='restaurants.txt', file_name_to_save='data_1.csv', n_sample=10)\n",
    "foody.start_crawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/banh-mi-pewpew-duong-d5\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/a-ma-kitchen-mon-hongkong\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/sasin-mi-cay-7-cap-do-han-quoc-310-nguyen-trong-tuyen\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/bun-thit-nuong-hai-dang\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/phuc-long-nowzone\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/banh-cuon-hai-nam\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/king-bbq-buffet-cao-thang\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/banh-canh-cua-tran-khac-chan\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/thanh-xuan-hu-tieu-my-tho\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/ech-xanh\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/ngo-do-bakery-banh-bo-thot-not-nguyen-son\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/che-buoi-vinh-long-2\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/thien-tan\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/kfc-xo-viet-nghe-tinh\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/pho-hung-nguyen-trai\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/ga-nuong-o-o-o-ga-nuong-nhieu-vi-ly-thuong-kiet\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/quan-nuong-co-khanh\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/bun-dau-homemade-phan-xich-long\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/king-bbq-buffet-le-van-sy\n",
      "\n",
      "Crawling data from link: https://www.foody.vn/ho-chi-minh/tra-sua-heekcaa-sai-gon-hoang-sa\n"
     ]
    }
   ],
   "source": [
    "foody = FoodyCrawler(browser_option=BrowserOption.EDGE, url_file='additional.txt', file_name_to_save='data_2_new.csv')\n",
    "foody.start_crawling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ffd7eb2cebf9ac436b5021ba01877e9cee6b03524e01bf8c8637d3e64111215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
